{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Extraction with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DonaldTrump = [\"He loves his enemies as himself so maybe do his enemies hearts good to wish him luck for 2020 @DonaldTrump  @EricTrump @DonaldTrumpJrA1\",\n",
    " \"Hey @DonaldTrump how about you get HOOKED ON PHONICS instead of being HOOKED ON ADDERALL #CognitiveTest\", \n",
    "\"@DonaldTrump Thank you for helping Albuquerque.  We are drowning in Soros funded A Holes.  Soros must be exposed.  Praying for you and us every day. Thx for Battle Hymn of the Republic at Mt. Rushmore !!\", \n",
    "\"Nice to c #Colombian support for @DonaldTrump .. Imagine a country enslaved by #Communists #Marsists  for 50 years.. Thank god they woke the f*ck up and kicked the  @FARC_COLOMBIA out to #Venezuela.. Please give them a FOLLOW. Thank you.\", \n",
    "\"@DonaldTrump Cat,Dog,Woman,Man,Camera ,TV,Idiot.#TrumpIsNotWell Dementia\", \n",
    "\"You definitely don't mean me.\", \n",
    "\"@DonaldTrump You can not switch it on and off 4 months out from the election. The American People's have gotten sick and tired of your   chaos and daily scandals .Wearing a Mask now  will not mask the damage you have done to this country .#FacistTrump\", \n",
    "\"7 day Facebook jail because the pic I posted of @donaldtrump and @ivanka VIOLATE community standards on sexually inappropriate pictures! THINK ABOUT IT\", \n",
    "\"No disputing the fact, @donaldtrump is a bloody RACIST.\", \n",
    "\"I'm glad @DonaldTrump enjoyed the MOCA so much and is raising awareness on a screen that we can't do from September as it is too expensive for our NHS. Thanks a bunch.\", \n",
    "\"Such a fucking idiot @DonaldTrump\", \n",
    "\"Why  @DonaldTrump don't go testify under oath about his friendship with Epstein and Maxwell !\", \n",
    "\"Here‚Äôs the real test you failed @donaldtrump, the American people. You have failed at every level possible. Hundreds of thousands have died at your hands due to a virus. American troops killed by your Russian allies. So many innocent lives lost because you‚Äôre a sociopath.\", \n",
    "\"No @BorisEP, this is happening right NOW under @realDonaldTrump, under @POTUS and under @donaldtrump and his administration. @JoeBiden is NOT the current President of the United States of America.\", \n",
    "\"The most astonishing thing about the attacks on @DanielAndrewsMP (many of them are warranted, btw), is that the most viscous emanate from @DonaldTrump supporters!\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ElonMusk = [\"Elon's most likely reply would be in binary.\", \n",
    "\"We‚Äôre starting to think about leaving California too... Sad but... ü§∑üèº‚Äç‚ôÇÔ∏èü§∑üèº‚Äç‚ôÇÔ∏èüôÜüèº‚Äç‚ôÇÔ∏è\", \n",
    "\"I don‚Äôt support any group that terrorizes people without the purpose of scaring those who incite violence. No I do not support ISIS.\", \n",
    "\"Sorry - I don't watch low-level reviews. China is not an exemplary country nor USA either.\", \n",
    "\"Not respectful of people but of their labor.\", \n",
    "\"Even if China double it's GHG emission, it's still like half of what Americans produce per capital base. Chinese don't live in a house with a swimming pool and 400m2 of lawn. What a hypocrite.\", \n",
    "\"The CDC data visualization is interactive. You can remove the ‚Äúassociated with Covid filter and other adjustments.\", \n",
    "\"Elon, Nebraska Bro, We need you build a factory here\", \n",
    "\"This entire argument is just a debate between religious fanatics. Comet worshipers vs void seekers.\", \n",
    "\"Hmmmm, that's one way to look at it. Cool. Anyway, up for human clinical trials of Neuralink ;) haha\", \n",
    "\"the internet has you on it man\", \n",
    "\"Oh, okay, so 40 down 40 back, so each car has to travel 80 miles in the span of 1 hour?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<12x142 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 172 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "sparse_matrix = count_vectorizer.fit_transform(ElonMusk)\n",
    "\n",
    "sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['elon', 'most', 'likely', 'reply', 'would', 'be', 'in', 'binary', 'we', 're', 'starting', 'to', 'think', 'about', 'leaving', 'california', 'too', 'sad', 'but', 'don', 'support', 'any', 'group', 'that', 'terrorizes', 'people', 'without', 'the', 'purpose', 'of', 'scaring', 'those', 'who', 'incite', 'violence', 'no', 'do', 'not', 'isis', 'sorry', 'watch', 'low', 'level', 'reviews', 'china', 'is', 'an', 'exemplary', 'country', 'nor', 'usa', 'either', 'respectful', 'their', 'labor', 'even', 'if', 'double', 'it', 'ghg', 'emission', 'still', 'like', 'half', 'what', 'americans', 'produce', 'per', 'capital', 'base', 'chinese', 'live', 'house', 'with', 'swimming', 'pool', 'and', '400m2', 'lawn', 'hypocrite', 'cdc', 'data', 'visualization', 'interactive', 'you', 'can', 'remove', 'associated', 'covid', 'filter', 'other', 'adjustments', 'nebraska', 'bro', 'need', 'build', 'factory', 'here', 'this', 'entire', 'argument', 'just', 'debate', 'between', 'religious', 'fanatics', 'comet', 'worshipers', 'vs', 'void', 'seekers', 'hmmmm', 'one', 'way', 'look', 'at', 'cool', 'anyway', 'up', 'for', 'human', 'clinical', 'trials', 'neuralink', 'haha', 'internet', 'has', 'on', 'man', 'oh', 'okay', 'so', '40', 'down', 'back', 'each', 'car', 'travel', '80', 'miles', 'span', 'hour']\n"
     ]
    }
   ],
   "source": [
    "print(list(count_vectorizer.vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "of           7\n",
       "the          5\n",
       "it           4\n",
       "in           3\n",
       "not          3\n",
       "to           3\n",
       "you          3\n",
       "don          3\n",
       "is           3\n",
       "has          2\n",
       "elon         2\n",
       "china        2\n",
       "but          2\n",
       "and          2\n",
       "people       2\n",
       "so           2\n",
       "support      2\n",
       "that         2\n",
       "40           2\n",
       "with         2\n",
       "we           2\n",
       "what         2\n",
       "too          1\n",
       "even         1\n",
       "covid        1\n",
       "data         1\n",
       "debate       1\n",
       "do           1\n",
       "who          1\n",
       "double       1\n",
       "down         1\n",
       "each         1\n",
       "either       1\n",
       "emission     1\n",
       "entire       1\n",
       "exemplary    1\n",
       "cool         1\n",
       "factory      1\n",
       "fanatics     1\n",
       "filter       1\n",
       "for          1\n",
       "ghg          1\n",
       "group        1\n",
       "haha         1\n",
       "half         1\n",
       "here         1\n",
       "hmmmm        1\n",
       "hour         1\n",
       "house        1\n",
       "country      1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix = sparse_matrix.todense()\n",
    "df = pd.DataFrame(doc_term_matrix, \n",
    "                  columns=count_vectorizer.get_feature_names())\n",
    "df.sum().sort_values(ascending=[False]).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'being', 'in', \"you'd\", 'just', 'no', 'isn', \"doesn't\", 'hadn', \"mightn't\", \"couldn't\", 'not', 'she', 'needn', 'themselves', 'further', 'same', \"aren't\", 'didn', 'doing', 'their', 'into', 'again', 'wasn', 'itself', 'that', 'up', 'can', \"mustn't\", 'doesn', 'a', 'theirs', 'aren', \"wasn't\", 're', 'under', 'too', 'what', 'been', 'him', 'did', 'o', 'or', 'below', 'ourselves', \"needn't\", 'with', 'so', 'me', 'such', \"isn't\", 'if', 'on', 'mustn', 'mightn', 'until', 'through', 'some', 'will', \"weren't\", 'your', \"won't\", 'her', 'over', 'wouldn', 'nor', 'shan', 'to', 'all', 've', 'here', \"shouldn't\", 'from', 'am', 'down', \"it's\", 'more', 'those', 'then', 'couldn', 'this', 'are', 'be', \"didn't\", 'above', \"you'll\", 'out', 'ma', \"she's\", 'when', 'his', 'few', 'who', 'where', 'most', 'by', 'for', 'my', 'while', 'about', \"that'll\", 'them', 'haven', \"shan't\", 'we', 'herself', 'both', 'very', 'now', \"hasn't\", 'have', \"you've\", 's', 'were', \"hadn't\", 'its', 'ours', 'was', 'll', 'they', 'yours', 'why', 'how', 'shouldn', 'than', 'don', 'our', 'these', 'any', 'should', 'once', 'as', 'an', 'myself', 'y', 'it', 'because', 'at', 't', \"should've\", 'won', 'but', 'each', \"wouldn't\", 'against', 'other', 'and', 'before', 'i', 'the', 'yourself', 'do', 'which', 'of', 'you', 'yourselves', 'is', 'm', 'own', \"don't\", 'having', \"haven't\", 'between', 'only', 'does', 'hasn', 'weren', 'ain', 'had', 'whom', 'himself', 'off', 'after', 'd', 'during', 'he', 'there', 'hers', 'has', \"you're\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/adrianacoca/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['elon',\n",
       " 'likely',\n",
       " 'reply',\n",
       " 'would',\n",
       " 'binary',\n",
       " 'starting',\n",
       " 'think',\n",
       " 'leaving',\n",
       " 'california',\n",
       " 'sad',\n",
       " 'support',\n",
       " 'group',\n",
       " 'terrorizes',\n",
       " 'people',\n",
       " 'without',\n",
       " 'purpose',\n",
       " 'scaring',\n",
       " 'incite',\n",
       " 'violence',\n",
       " 'isis',\n",
       " 'sorry',\n",
       " 'watch',\n",
       " 'low',\n",
       " 'level',\n",
       " 'reviews',\n",
       " 'china',\n",
       " 'exemplary',\n",
       " 'country',\n",
       " 'usa',\n",
       " 'either',\n",
       " 'respectful',\n",
       " 'labor',\n",
       " 'even',\n",
       " 'double',\n",
       " 'ghg',\n",
       " 'emission',\n",
       " 'still',\n",
       " 'like',\n",
       " 'half',\n",
       " 'americans',\n",
       " 'produce',\n",
       " 'per',\n",
       " 'capital',\n",
       " 'base',\n",
       " 'chinese',\n",
       " 'live',\n",
       " 'house',\n",
       " 'swimming',\n",
       " 'pool',\n",
       " '400m2',\n",
       " 'lawn',\n",
       " 'hypocrite',\n",
       " 'cdc',\n",
       " 'data',\n",
       " 'visualization',\n",
       " 'interactive',\n",
       " 'remove',\n",
       " 'associated',\n",
       " 'covid',\n",
       " 'filter',\n",
       " 'adjustments',\n",
       " 'nebraska',\n",
       " 'bro',\n",
       " 'need',\n",
       " 'build',\n",
       " 'factory',\n",
       " 'entire',\n",
       " 'argument',\n",
       " 'debate',\n",
       " 'religious',\n",
       " 'fanatics',\n",
       " 'comet',\n",
       " 'worshipers',\n",
       " 'vs',\n",
       " 'void',\n",
       " 'seekers',\n",
       " 'hmmmm',\n",
       " 'one',\n",
       " 'way',\n",
       " 'look',\n",
       " 'cool',\n",
       " 'anyway',\n",
       " 'human',\n",
       " 'clinical',\n",
       " 'trials',\n",
       " 'neuralink',\n",
       " 'haha',\n",
       " 'internet',\n",
       " 'man',\n",
       " 'oh',\n",
       " 'okay',\n",
       " '40',\n",
       " 'back',\n",
       " 'car',\n",
       " 'travel',\n",
       " '80',\n",
       " 'miles',\n",
       " 'span',\n",
       " 'hour']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)\n",
    "\n",
    "tokens_clean = [e for e in list(count_vectorizer.vocabulary_.keys()) if e not in stop_words]\n",
    "tokens_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Extraction with SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Named Entity Recognition (NER)** - Labelling named ‚Äúreal-world‚Äù objects, like persons, companies or locations.  \n",
    "**Text Classification** - Assigning categories or labels to a whole document, or parts of a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f3581bbb6f69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_char\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_char\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "\n",
    "doc = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "\n",
    "for ent in doc:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
